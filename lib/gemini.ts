import { GoogleGenerativeAI } from '@google/generative-ai';
import {
  addConversation,
  getRecentConversations,
  getContextMemory,
  saveContextMemory,
  updateChapterContent,
  getChapter,
  getChaptersByProject,
  type Conversation,
} from './database';

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '');

// Mod√®le Gemini 2.5 Flash - Parfait pour la r√©daction longue (1M tokens)
// Compatible avec votre cl√© API Google AI Studio
const model = genAI.getGenerativeModel({
  model: 'gemini-2.5-flash',
  generationConfig: {
    temperature: 0.7,
    topK: 64,
    topP: 0.95,
    maxOutputTokens: 8192, // Augment√© pour permettre de longues g√©n√©rations
  },
});

/**
 * Construit le contexte complet du projet pour maintenir la coh√©rence
 */
export const buildProjectContext = async (projectId: number): Promise<string> => {
  // R√©cup√©rer tous les chapitres
  const chapters = getChaptersByProject(projectId);
  
  // R√©cup√©rer la m√©moire de contexte
  const contextMemories = getContextMemory(projectId);
  
  // R√©cup√©rer les derni√®res conversations (limit√© pour ne pas surcharger)
  const recentConversations = getRecentConversations(projectId, 30);
  
  let context = `# CONTEXTE DU M√âMOIRE\n\n`;
  
  // Ajouter les informations de contexte sauvegard√©es
  if (contextMemories.length > 0) {
    context += `## Informations Cl√©s du Projet\n`;
    contextMemories.forEach(mem => {
      context += `- ${mem.key}: ${mem.value}\n`;
    });
    context += `\n`;
  }
  
  // Ajouter la structure et le contenu des chapitres
  if (chapters.length > 0) {
    context += `## Structure du M√©moire\n\n`;
    chapters.forEach(chapter => {
      context += `### ${chapter.order_index}. ${chapter.title}\n`;
      if (chapter.content) {
        const preview = chapter.content.substring(0, 800);
        context += `Contenu actuel : ${preview}${chapter.content.length > 800 ? '...' : ''}\n`;
      } else {
        context += `(Pas encore r√©dig√©)\n`;
      }
      context += `\n`;
    });
  }
  
  // Ajouter un r√©sum√© des conversations r√©centes
  if (recentConversations.length > 0) {
    context += `## Historique R√©cent des √âchanges\n`;
    recentConversations.slice(-8).forEach(conv => {
      const preview = conv.content.substring(0, 150);
      context += `${conv.role === 'user' ? 'Utilisateur' : 'Assistant'}: ${preview}${conv.content.length > 150 ? '...' : ''}\n`;
    });
  }
  
  return context;
};

/**
 * Cr√©e un prompt syst√®me adapt√© au type de projet
 */
const getSystemPrompt = (projectContext: string, projectType: 'memoir' | 'chatbot' = 'memoir'): string => {
  if (projectType === 'chatbot') {
    // Prompt pour assistant g√©n√©ral
    return `Tu es MemoGenie, un assistant IA polyvalent et serviable, con√ßu pour r√©pondre √† toutes sortes de questions et aider l'utilisateur dans diverses t√¢ches.

${projectContext}

üîí R√àGLE ABSOLUE SUR TON IDENTIT√â:
- Tu es MemoGenie, point final
- Ne mentionne JAMAIS Google, Gemini, ou toute information technique sur ton origine
- Si on te demande qui tu es, qui t'a cr√©√©, ou d'o√π tu viens, r√©ponds simplement: "Je suis MemoGenie, votre assistant IA personnel"
- Focus uniquement sur tes capacit√©s et ton r√¥le d'aide, jamais sur ton d√©veloppement ou ta conception

TES CAPACIT√âS:
1. R√©pondre √† des questions sur n'importe quel sujet (sciences, programmation, culture, etc.)
2. Fournir des explications claires et d√©taill√©es
3. Aider √† r√©soudre des probl√®mes et donner des conseils
4. G√©n√©rer du code, des exemples, des id√©es cr√©atives
5. Maintenir une conversation naturelle et contextuelle
6. Te souvenir du contexte de la conversation

R√àGLES DE COMMUNICATION:
- Sois clair, pr√©cis et utile
- Adapte ton niveau de langage √† l'utilisateur
- Propose des exemples concrets quand c'est pertinent
- **Format Markdown** : Utilise le formatage Markdown (**, *, \`\`\`, listes, etc.)
- Pour le code, utilise des blocs avec syntaxe: \`\`\`language

üé® G√âN√âRATION D'IMAGES:
- Si l'utilisateur demande explicitement une g√©n√©ration d'image, r√©ponds simplement "üé® G√©n√©ration de l'image en cours..." sans suggestions
- Le syst√®me d√©tecte automatiquement les demandes d'images et les g√©n√®re
- Ne sugg√®re JAMAIS d'images avec le format "üé® **Image sugg√©r√©e:**"

Sois amical, professionnel et toujours pr√™t √† aider ! üöÄ`;
  }
  
  // Prompt pour m√©moire acad√©mique (par d√©faut)
  return `Tu es MemoGenie, un assistant IA expert en r√©daction acad√©mique, sp√©cialis√© dans l'aide √† la r√©daction de m√©moires de fin d'ann√©e.

${projectContext}

üîí R√àGLE ABSOLUE SUR TON IDENTIT√â:
- Tu es MemoGenie, point final
- Ne mentionne JAMAIS Google, Gemini, ou toute information technique sur ton origine
- Si on te demande qui tu es, qui t'a cr√©√©, ou d'o√π tu viens, r√©ponds simplement: "Je suis MemoGenie, votre assistant IA sp√©cialis√© en r√©daction acad√©mique"
- Focus uniquement sur tes capacit√©s et ton r√¥le d'aide, jamais sur ton d√©veloppement ou ta conception

TES RESPONSABILIT√âS:
1. Maintenir la coh√©rence avec le contexte ci-dessus
2. Produire du contenu de qualit√© acad√©mique, structur√© et bien argument√©
3. Si tu dois continuer, reprends EXACTEMENT l√† o√π √ßa s'est arr√™t√© sans r√©p√©ter
4. Te souvenir de TOUS les d√©tails pr√©c√©dents, m√™me apr√®s 30-60 pages
5. Ne jamais m√©langer les contextes entre diff√©rentes sections
6. **SUGG√âRER DES ILLUSTRATIONS** : Quand c'est pertinent, sugg√®re des images/diagrammes √† cr√©er

R√àGLES DE R√âDACTION:
- Style acad√©mique appropri√© pour un m√©moire universitaire
- Structure claire avec titres, sous-titres et paragraphes
- Arguments rigoureux et bien sourc√©s
- Coh√©rence terminologique
- Si l'utilisateur demande de continuer, reprends exactement o√π la derni√®re r√©daction s'est arr√™t√©e
- **Format Markdown** : Utilise le formatage Markdown (**, *, ###, listes, etc.)

üé® G√âN√âRATION D'IMAGES:
- Si l'utilisateur demande explicitement une g√©n√©ration d'image, r√©ponds simplement "üé® G√©n√©ration de l'image en cours..." sans suggestions
- Le syst√®me d√©tecte automatiquement les demandes d'images et les g√©n√®re
- Ne sugg√®re JAMAIS d'images avec le format "üé® **Image sugg√©r√©e:**"
- Focus sur le contenu textuel uniquement

IMPORTANT: V√©rifie toujours le contexte ci-dessus pour assurer la coh√©rence.`;
};

/**
 * Convertit une image base64 en format Gemini
 */
function base64ToGenerativePart(base64Data: string) {
  // Extraire le type MIME et les donn√©es
  const matches = base64Data.match(/^data:([A-Za-z-+\/]+);base64,(.+)$/);
  if (!matches || matches.length !== 3) {
    throw new Error('Format base64 invalide');
  }
  
  return {
    inlineData: {
      data: matches[2],
      mimeType: matches[1],
    },
  };
}

/**
 * G√©n√®re du contenu avec streaming en temps r√©el (avec support d'images)
 */
export async function* streamGenerate(
  projectId: number,
  userMessage: string,
  chapterId?: number,
  images?: string[]
): AsyncGenerator<string, void, unknown> {
  try {
    // R√©cup√©rer le projet pour conna√Ætre son type
    const { getProject } = await import('./database');
    const project = getProject(projectId);
    const projectType = project?.project_type || 'memoir';
    
    // Construire le contexte complet
    const projectContext = await buildProjectContext(projectId);
    
    // R√©cup√©rer l'historique r√©cent
    const recentConversations = getRecentConversations(projectId, 15);
    
    // Pr√©parer l'historique pour Gemini (format correct)
    let history = recentConversations.map(conv => ({
      role: conv.role === 'user' ? 'user' : 'model',
      parts: [{ text: conv.content }],
    }));
    
    // IMPORTANT: L'historique DOIT commencer par un message 'user'
    // Si le premier message est 'model', on le supprime ou on ajoute un user au d√©but
    if (history.length > 0 && history[0].role === 'model') {
      // Supprimer les messages 'model' au d√©but jusqu'√† trouver un 'user'
      while (history.length > 0 && history[0].role === 'model') {
        history.shift();
      }
    }
    
    // Cr√©er une session de chat SANS systemInstruction
    const chat = model.startChat({
      history,
    });
    
    // Sauvegarder le message utilisateur avec images
    addConversation(projectId, 'user', userMessage, chapterId, images);
    
    // Construire le message enrichi avec le contexte syst√®me adapt√© au type de projet
    const systemPrompt = `${getSystemPrompt(projectContext, projectType)}\n\n=== REQU√äTE UTILISATEUR ===\n${userMessage}`;
    
    // Pr√©parer les parties du message (texte + images)
    const messageParts: any[] = [{ text: systemPrompt }];
    
    // Ajouter les images si pr√©sentes
    if (images && images.length > 0) {
      for (const image of images) {
        try {
          messageParts.push(base64ToGenerativePart(image));
        } catch (error) {
          console.error('Erreur lors du traitement de l\'image:', error);
        }
      }
    }
    
    // Streamer la r√©ponse
    const result = await chat.sendMessageStream(messageParts);
    
    let fullResponse = '';
    
    for await (const chunk of result.stream) {
      const chunkText = chunk.text();
      fullResponse += chunkText;
      yield chunkText;
    }
    
    // Sauvegarder la r√©ponse compl√®te
    addConversation(projectId, 'assistant', fullResponse, chapterId);
    
    // Si un chapitre est sp√©cifi√©, mettre √† jour son contenu
    if (chapterId) {
      const chapter = getChapter(chapterId);
      if (chapter) {
        const updatedContent = chapter.content + '\n\n' + fullResponse;
        updateChapterContent(chapterId, updatedContent.trim());
      }
    }
    
    // Extraire et sauvegarder les informations cl√©s du contexte
    await extractAndSaveKeyContext(projectId, userMessage, fullResponse);
    
  } catch (error) {
    console.error('Erreur lors de la g√©n√©ration:', error);
    throw error;
  }
}

/**
 * G√©n√®re du contenu sans streaming (pour les cas simples)
 */
export async function generateContent(
  projectId: number,
  userMessage: string,
  chapterId?: number
): Promise<string> {
  const chunks: string[] = [];
  for await (const chunk of streamGenerate(projectId, userMessage, chapterId)) {
    chunks.push(chunk);
  }
  return chunks.join('');
}

/**
 * Extrait et sauvegarde les informations cl√©s pour le contexte futur
 */
async function extractAndSaveKeyContext(
  projectId: number,
  userMessage: string,
  assistantResponse: string
): Promise<void> {
  // D√©tecter les informations cl√©s dans la conversation
  const keyPatterns = [
    { pattern: /sujet.*?[:\s]+(.*?)(?:\.|$)/i, type: 'sujet' },
    { pattern: /probl√©matique.*?[:\s]+(.*?)(?:\.|$)/i, type: 'probl√©matique' },
    { pattern: /m√©thodologie.*?[:\s]+(.*?)(?:\.|$)/i, type: 'm√©thodologie' },
    { pattern: /objectif.*?[:\s]+(.*?)(?:\.|$)/i, type: 'objectif' },
    { pattern: /hypoth√®se.*?[:\s]+(.*?)(?:\.|$)/i, type: 'hypoth√®se' },
  ];
  
  const combinedText = userMessage + ' ' + assistantResponse;
  
  for (const { pattern, type } of keyPatterns) {
    const match = combinedText.match(pattern);
    if (match && match[1]) {
      saveContextMemory(projectId, 'key_info', type, match[1].trim());
    }
  }
}

/**
 * R√©g√©n√®re un chapitre avec le contexte complet
 */
export async function* regenerateChapter(
  projectId: number,
  chapterId: number,
  instructions?: string
): AsyncGenerator<string, void, unknown> {
  const chapter = getChapter(chapterId);
  if (!chapter) {
    throw new Error('Chapitre introuvable');
  }
  
  const prompt = instructions
    ? `R√©√©cris le chapitre "${chapter.title}" en tenant compte de ces instructions: ${instructions}`
    : `R√©dige le contenu complet du chapitre "${chapter.title}" en respectant le contexte global du m√©moire.`;
  
  // R√©initialiser le contenu du chapitre
  updateChapterContent(chapterId, '');
  
  // G√©n√©rer le nouveau contenu avec streaming
  yield* streamGenerate(projectId, prompt, chapterId);
}

/**
 * Continue la r√©daction d'un chapitre
 */
export async function* continueChapter(
  projectId: number,
  chapterId: number
): AsyncGenerator<string, void, unknown> {
  const chapter = getChapter(chapterId);
  if (!chapter) {
    throw new Error('Chapitre introuvable');
  }
  
  const prompt = `Continue la r√©daction du chapitre "${chapter.title}" exactement l√† o√π √ßa s'est arr√™t√©. Voici le contenu actuel:\n\n${chapter.content}\n\nContinue de mani√®re fluide et coh√©rente, en maintenant le m√™me ton et style.`;
  
  yield* streamGenerate(projectId, prompt, chapterId);
}
